# This file contains sets of arguments that, when read by
# the param_search.sh script, run the specified commands on
# whatever GPU happens to be free and continues until
# all lines are run.
#
# Current lstm relation parameter arguments and defaults are
# epochs                20
# batch_size            100
# lstm_hidden_width     200
# start_hidden_width    150
# weighted_classes      False
# hidden_depth          1
# learn_rate            0.001
# adam_epsilon          1e-08
# clip_norm             5.0
# data_norm             False
# input_keep_prob       1.0
# other_keep_prob       1.0
# pair_enc_scheme       fist_last_sentence
# activation            relu


# How does the system work with the other paper's dropout settings?
#--input_keep_prob=0.8 --other_keep_prob=0.5

# How does that compare when we use first_avg_last pair encoding?
#--pair_enc_scheme=first_avg_last --input_keep_prob=0.8 --other_keep_prob=0.5

# What if we incorporate leaky relu?
#--input_keep_prob=0.8 --other_keep_prob=0.5 --activation=leaky_relu

# What happens when we increase the depth to 2? With larger hidden layers?
#--input_keep_prob=0.8 --other_keep_prob=0.5 --start_hidden_width=300 --hidden_depth=2
#--input_keep_prob=0.8 --other_keep_prob=0.5 --start_hidden_width=600 --hidden_depth=2

# How does decreasing the learning rate by 50%? By 90%?
#--input_keep_prob=0.8 --other_keep_prob=0.5 --learn_rate=0.0005
#--input_keep_prob=0.8 --other_keep_prob=0.5 --learn_rate=0.0001

# What if we increase the dropout rate on both the inputs and the hidden units? Just the inputs? Just the hidden units?
#--input_keep_prob=0.9 --other_keep_prob=0.75
#--input_keep_prob=0.9 --other_keep_prob=0.5
#--input_keep_prob=0.8 --other_keep_prob=0.75


# All of the above were generally pretty stable options, so we're going to try to play
# around with a few of the defaults to make sure they're as good as we think they are,
# in addition to incorporating some of the good params we've seen thus far; for each,
# we're only going to train ten epochs, since that's typically all we need

# Let's not use dropout at all
#--epochs=10

# Only use dropout on the hidden layers but not the input
#--epochs=10 --other_keep_prob=0.5

# Use a (relatively low) dropout rate on the inputs
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.5

# Going back to our 'default' dropout settings, what happens when we actually
# run with three hidden layers? 4?
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --start_hidden_width=600 --hidden_depth=3
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --start_hidden_width=600 --hidden_depth=4

# Are weighted classes an idea worth pursuing?
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --weighted_classes

# What about data normalization?
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --data_norm

# How does clip size effect our results?
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --clip_norm=1.0
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --clip_norm=10.0

# The tensorflow documentation suggests that the default adam epsilon may be
# inappropriate for some cases. Try halving it, along with their alternatives
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --adam_epsilon=0.00001
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --adam_epsilon=0.1
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --adam_epsilon=1.0

# Much as I'm loathe to do so, double check our batch size again; And try powers of 2 this time
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --batch_size=32
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --batch_size=64
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --batch_size=128
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --batch_size=256

# Let's take all of the best settings from our first batch of experiments and see where we're at
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --activation=leaky_relu --start_hidden_width=300 --hidden_depth=2 --learn_rate=0.0005
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --start_hidden_width=300 --hidden_depth=2 --learn_rate=0.0005
#--epochs=10 --other_keep_prob=0.5 --input_keep_prob=0.8 --pair_enc_scheme=first_avg_last --start_hidden_width=300 --hidden_depth=2 --learn_rate=0.0005


# After comparison of results across settings and intra/cross relations, let's try the following
# focusing on dropout, hidden width/depth, data norm,, and batch size. And let's do it all on the
# full data for 100 epochs, assuming I've implemented early stopping
--epochs=100 --input_keep_prob=0.5 --other_keep_prob=0.5 --lstm_hidden_width=200 --start_hidden_width=1024 --hidden_depth=3 --data_norm --batch_size=512
--epochs=100 --input_keep_prob=0.5 --other_keep_prob=0.5 --lstm_hidden_width=200 --start_hidden_width=1024 --hidden_depth=3 --data_norm --batch_size=256
--epochs=100 --input_keep_prob=0.5 --other_keep_prob=0.5 --lstm_hidden_width=200 --start_hidden_width=512 --hidden_depth=3 --data_norm --batch_size=512
--epochs=100 --input_keep_prob=0.5 --other_keep_prob=0.5 --lstm_hidden_width=200 --start_hidden_width=512 --hidden_depth=3 --data_norm --batch_size=256
--epochs=100 --input_keep_prob=0.5 --other_keep_prob=0.5 --lstm_hidden_width=200 --start_hidden_width=512 --hidden_depth=2 --data_norm --batch_size=512
--epochs=100 --input_keep_prob=0.5 --other_keep_prob=0.5 --lstm_hidden_width=200 --start_hidden_width=512 --hidden_depth=2 --data_norm --batch_size=256


